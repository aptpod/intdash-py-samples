{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieve physical values in real time and apply a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we will focus on the following methods:\n",
    "\n",
    "- Retrieve real time data from another edge\n",
    "- Apply a function to the retrieved data\n",
    "- Upload the processed data in real time\n",
    "\n",
    "## Scenario\n",
    "Use the iOS application **intdash Motion**.\n",
    "Receive the streamed data from your iPhone, process the sensor data with `intdash-py`, and upload it to the server. In this scenario, we will use a moving average as an example of a function to be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before starting this scenario, prepare the following.\n",
    "\n",
    "- An edge that performs measurement\n",
    "- intdash Motion Application\n",
    "- Signal definition for general sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to be used\n",
    "In this scenario, the following data needs to be prepared on the server side.\n",
    "\n",
    "| Data item | Data name that appears in this scenario |\n",
    "|:---|:---|\n",
    "|Edge for data acquisition|sdk_edge1|\n",
    "|Edge for data upload|sdk_edge2|\n",
    "| Signal definition(\\*) | `sp_ACCX`, `sp_ACCY`, `sp_ACCZ`|\n",
    "\n",
    "(\\*) Use the same signal definition used in **\"1. Retrieve time series data and save as CSV\"**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and create a client\n",
    "For `url` given to ` intdash.Client`, specify the environment of the intdash server. For `edge_token`, specify the token issued for the edge you use.\n",
    "(\\* Login with `username` and `password` is also possible, but it is recommended to use edge token for continued use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import intdash\n",
    "from intdash import timeutils\n",
    "\n",
    "# Create client\n",
    "client = intdash.Client(\n",
    "    url = \"https://example.intdash.jp\",\n",
    "    edge_token = \"your_token\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the signal definition\n",
    "Use the same signal definition used in **\"1. Retrieve time series data and save as CSV\"**.\n",
    "See the following file for how to convert general sensor type data to numerical values.\n",
    "\n",
    "[create-signal-general-sensor.ipynb](../0_create-signals/create-signal-general-sensor.ipynb)\n",
    "\n",
    "In this sample, only the conversion definition of \"acceleration\" is registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that the signal definitions are registered\n",
    "Confirm that the signal definitions for this scenario are registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = client.signals.list(label='sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_ACCX, sp_ACCY, sp_ACCZ, "
     ]
    }
   ],
   "source": [
    "for s in signals:\n",
    "    print(s.label,  end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk_edge1 = client.edges.list(name='sdk_edge1')[0]\n",
    "sdk_edge2 = client.edges.list(name='sdk_edge2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sdk_edge1', 'sdk_edge2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdk_edge1.name, sdk_edge2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Queue\n",
    "The process from download to upload is performed using a Queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "q = queue.Queue(maxsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of data retrieval (Downstream)\n",
    "Define a process to receive time series data from an edge through the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a request\n",
    "For ``src_edge_uuid``, specify the source edge. In this example, the edge ``sdk_edge1`` running intdash Motion is used.\n",
    "Specify the ``label`` name of the signal definition in `data_id` of `core.DataFilter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_specs = [\n",
    "        intdash.DownstreamSpec(\n",
    "            src_edge_uuid = sdk_edge1.uuid, #edge_uuid\n",
    "            filters = [\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCX',channel=1),  # Acceleration X\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCY',channel=1),  # Acceleration Y\n",
    "                 intdash.DataFilter(data_type=intdash.DataType.float.value, data_id='sp_ACCZ',channel=1),  # Acceleration Z\n",
    "            ],\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to be applied to the received data\n",
    "Define a process to receive time series data and add it to the Queue as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the received time-series data to the Queue.\n",
    "def callback(unit):\n",
    "    try:\n",
    "        q.put_nowait(unit)\n",
    "    except queue.Full:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for data upload (Upstream)\n",
    "Define a function that processes the received data and sends the data back to the server as new time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a request\n",
    "Specify the UUID of the uploading edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_specs = [\n",
    "        intdash.UpstreamSpec(\n",
    "            src_edge_uuid = sdk_edge2.uuid,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the process\n",
    "In this scenario, define the process that calculates the moving average and returns it to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The function calculate moving average.\n",
    "def calc_ave(score, array, ave_num):\n",
    "    array.append(score)\n",
    "    if len(array) > ave_num:\n",
    "        array.popleft()\n",
    "  \n",
    "    return  np.sum(array)/ len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an uploader function\n",
    "\n",
    "Define the following processes:\n",
    "\n",
    "- Get data from the Queue\n",
    "- Switch the process depending on the data type of time series data\n",
    "- Put data into the process\n",
    "- Send the newly created data back (return the intdash.Unit with \"yield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVE_NUM = 5\n",
    "\n",
    "import struct\n",
    "from collections import deque\n",
    "\n",
    "acc_x_dq = deque([])\n",
    "acc_y_dq = deque([])\n",
    "acc_z_dq = deque([])\n",
    "\n",
    "\n",
    "# Calculate moving average of the received time-series data,  convert it to Unit and upload. \n",
    "def upload_func():\n",
    "    while True:\n",
    "        try:\n",
    "            unit = q.get_nowait()\n",
    "            \n",
    "            # Skip basetime.\n",
    "            if unit.data.data_type.value == intdash.DataType.basetime.value:\n",
    "                yield unit\n",
    "                continue\n",
    "            \n",
    "            # Skip other data.\n",
    "            if unit.data.data_type.value != intdash.DataType.float.value:\n",
    "                yield unit\n",
    "                continue\n",
    "                \n",
    "            # Get intdash.intdash.data.GeneralSensor \n",
    "            sensor_data = unit.data\n",
    "                \n",
    "            if unit.data.data_id == 'sp_ACCX':\n",
    "                acc_x = unit.data.value\n",
    "                ave_acc_x = calc_ave(acc_x, acc_x_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_x is None:\n",
    "                    continue\n",
    "               \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data =  intdash.data.Float(data_id = 'sp_ACCX', value =ave_acc_x ),\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "                 \n",
    "            if unit.data.data_id == 'sp_ACCY':\n",
    "                acc_y = unit.data.value\n",
    "                ave_acc_y = calc_ave(acc_y, acc_y_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_y is None:\n",
    "                    continue\n",
    "               \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data =  intdash.data.Float(data_id = 'sp_ACCY', value =ave_acc_y ),\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "            if unit.data.data_id == 'sp_ACCZ':\n",
    "                acc_z = unit.data.value\n",
    "                ave_acc_z = calc_ave(acc_z, acc_z_dq, AVE_NUM)\n",
    "                \n",
    "                if ave_acc_z is None:\n",
    "                    continue\n",
    "                \n",
    "                yield intdash.Unit(\n",
    "                      elapsed_time = unit.elapsed_time,\n",
    "                      channel = 1,\n",
    "                      data = intdash.data.Float(data_id = 'sp_ACCZ', value =ave_acc_z ),\n",
    "                    )\n",
    "                \n",
    "                continue\n",
    "                \n",
    "        except queue.Empty:\n",
    "            yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn = client.connect_websocket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.open_upstreams(\n",
    "    specs = u_specs,\n",
    "    iterators = [upload_func()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.open_downstreams(\n",
    "    specs = d_specs,\n",
    "    callbacks = [callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disconnect\n",
    "If you want to end the process, be sure to execute the following to disconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data with Visual M2M Data Visualizer\n",
    "By using **Visual M2M Data Visualizer**, you can confirm that the data is being communicated in real time. If you import the \"SCREEN file (.scrn)\" and \"DAT file (.dat)\" in the same directory as this notebook into **Visual M2M Data Visualizer**, you can check the data as follows. (For details, see the Operation Manual of **Visual M2M Data Visualizer**.)\n",
    " \n",
    " \n",
    "On the screen below, `Acceleration raw` panel shows the data before conversion (data sent by Motion app), and `Acceleration Converted` panel shows the moving average calculated using `intdash-py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/aptpod/intdash-py-sample-codes/blob/master/img/img3.png?raw=true\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
